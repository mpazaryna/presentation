---
marp: true
theme: default
style: |
  section {
    font-size: 28px;
  }
---

# MLX Framework

Machine Learning on Apple Silicon

---

# The Landscape

ML frameworks optimized for NVIDIA GPUs

Apple Silicon left behind

Gap in the market

---

# Enter MLX

Open-source from Apple

Python-first design

Unified memory architecture

---

# Why Apple Silicon?

Billions of devices

High-performance GPU

Shared memory model

Massive untapped potential

---

# Core Design Principles

Simple and intuitive API

Familiar NumPy-like syntax

Lazy evaluation

Composable functions

---

# Architecture Overview

NumPy-like array interface

Unified memory (CPU + GPU)

Automatic differentiation

JIT compilation

---

# Key Features

Efficient inference on-device

Fine-tuning and training

Memory-optimized operations

Cross-platform compatibility

---

# MLX vs Alternatives

PyTorch: GPU-focused, larger overhead

TensorFlow: Production-heavy, complex

JAX: Functional, steep learning curve

MLX: Apple Silicon native, lightweight

---

# Core Concepts

**Arrays:** Lazy computation model

**Operations:** GPU-accelerated when beneficial

**Gradients:** Full autodiff support

**Training:** Lightning-fast fine-tuning

---

# What You Can Do

Inference on M-series chips

Train models end-to-end

Fine-tune LLMs locally

Run quantized models efficiently

---

# Getting Started

Install: pip install mlx

Documentation: mlx-framework.readthedocs.io

Examples: github.com/ml-explore/mlx

Community: Active development

---

# The Future

On-device AI becomes standard

Privacy-first applications

Distributed ML at the edge

---

# Questions?

**MLX:** github.com/ml-explore/mlx